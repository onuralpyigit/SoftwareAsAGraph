{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             QualifiedName                            Name   \n",
      "1           <Package>org.argouml.activity2           org.argouml.activity2  \\\n",
      "2   <Package>org.argouml.activity2.diagram   org.argouml.activity2.diagram   \n",
      "3         <Package>org.argouml.application         org.argouml.application   \n",
      "4     <Package>org.argouml.application.api     org.argouml.application.api   \n",
      "5  <Package>org.argouml.application.events  org.argouml.application.events   \n",
      "\n",
      "   Complexity  Coupling  Size  Lack of Cohesion  #(C&I)    #C   #I    LOC   \n",
      "1           1         1     1                 1     3.0   3.0  0.0   39.0  \\\n",
      "2           1         4     3                 1    30.0  26.0  4.0  847.0   \n",
      "3           1         1     2                 1     6.0   6.0  0.0  476.0   \n",
      "4           1         1     2                 1     7.0   2.0  5.0  170.0   \n",
      "5           1         2     3                 1    16.0   9.0  7.0  353.0   \n",
      "\n",
      "     AC    EC    Abs    Ins     ND    WMC  \n",
      "1   0.0   2.0  0.000  1.000  0.000   12.0  \n",
      "2   1.0  23.0  0.133  0.958  0.091  187.0  \n",
      "3   0.0   4.0  0.000  1.000  0.000   97.0  \n",
      "4  91.0   2.0  0.857  0.022  0.121   31.0  \n",
      "5  41.0   7.0  0.500  0.146  0.354  121.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t')\n",
    "    return data\n",
    "\n",
    "def remove_empty_rows(data):\n",
    "    data = data.dropna(subset=['Name'])\n",
    "    return data\n",
    "\n",
    "def remove_unnamed_columns(data):\n",
    "    data = data.filter(regex='^(?!Unnamed.*)')\n",
    "    return data\n",
    "\n",
    "def remove_nan_columns(data):\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    return data\n",
    "\n",
    "def convert_ordinal_to_numerical(data):\n",
    "    ordinal_mapping = {\n",
    "        'low': 1,\n",
    "        'low-medium': 2,\n",
    "        'medium-high': 3,\n",
    "        'high': 4,\n",
    "        'very-high': 5\n",
    "    }\n",
    "    columns_to_convert = ['Complexity', 'Coupling', 'Size', 'Lack of Cohesion']\n",
    "    for column in columns_to_convert:\n",
    "        data[column] = data[column].replace(ordinal_mapping)\n",
    "    return data\n",
    "\n",
    "#project_name = \"jfreechart\"\n",
    "project_name = \"argouml\"\n",
    "dataset_file = \"input/\" + project_name + \"/package-metrics-dataset.csv\"\n",
    "dataset = load_data(dataset_file)\n",
    "dataset = remove_empty_rows(dataset)\n",
    "dataset = remove_unnamed_columns(dataset)\n",
    "dataset = remove_nan_columns(dataset)\n",
    "dataset = convert_ordinal_to_numerical(dataset)\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 93\n",
      "Number of Problematic Components in the dataset: 14\n",
      "Number of Highly Problematic Components in the dataset: 1\n"
     ]
    }
   ],
   "source": [
    "def label_problematic_component(row, problematic_class_threshold=3, highly_problematic_class_threshold=4):\n",
    "    problematic_score = (row['Complexity'] + row['Coupling'] + row['Size'] + row['Lack of Cohesion']) / 4\n",
    "    if problematic_score >= highly_problematic_class_threshold:\n",
    "        return 2\n",
    "    elif problematic_score >= problematic_class_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Label the dataset\n",
    "dataset['Problematic'] = dataset.apply(label_problematic_component, axis=1)\n",
    "\n",
    "# Count the number of Problematic Classes\n",
    "num_dataset_rows = dataset.shape[0]\n",
    "num_problematic_classes = dataset[dataset['Problematic'] == 1].shape[0]\n",
    "num_highly_problematic_classes = dataset[dataset['Problematic'] == 2].shape[0]\n",
    "print(f'Number of instances in the dataset: {num_dataset_rows}')\n",
    "print(f'Number of Problematic Components in the dataset: {num_problematic_classes}')\n",
    "print(f'Number of Highly Problematic Components in the dataset: {num_highly_problematic_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Name  #(C&I)    #C   #I    LOC    AC    EC   \n",
      "1           org.argouml.activity2     3.0   3.0  0.0   39.0   0.0   2.0  \\\n",
      "2   org.argouml.activity2.diagram    30.0  26.0  4.0  847.0   1.0  23.0   \n",
      "3         org.argouml.application     6.0   6.0  0.0  476.0   0.0   4.0   \n",
      "4     org.argouml.application.api     7.0   2.0  5.0  170.0  91.0   2.0   \n",
      "5  org.argouml.application.events    16.0   9.0  7.0  353.0  41.0   7.0   \n",
      "\n",
      "     Abs    Ins     ND    WMC  Problematic  \n",
      "1  0.000  1.000  0.000   12.0            0  \n",
      "2  0.133  0.958  0.091  187.0            0  \n",
      "3  0.000  1.000  0.000   97.0            0  \n",
      "4  0.857  0.022  0.121   31.0            0  \n",
      "5  0.500  0.146  0.354  121.0            0  \n"
     ]
    }
   ],
   "source": [
    "def remove_labeling_columns(data):\n",
    "    data = data.drop(['QualifiedName', 'Complexity', 'Coupling', 'Size', 'Lack of Cohesion'], axis=1)\n",
    "    return data\n",
    "\n",
    "dataset = remove_labeling_columns(dataset)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    X = data.drop(['Name', 'Problematic'], axis=1)\n",
    "    y = data['Problematic']\n",
    "    return X, y\n",
    "\n",
    "X, y = preprocess_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_decision_tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf, X_test, y_test\n",
    "\n",
    "clf, X_test, y_test = train_decision_tree(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(clf, X_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "y_pred = make_predictions(clf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[16  1  0]\n",
      " [ 0  1  0]\n",
      " [ 0  1  0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        17\n",
      "           1       0.33      1.00      0.50         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.89        19\n",
      "   macro avg       0.44      0.65      0.49        19\n",
      "weighted avg       0.91      0.89      0.89        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_results(y_test, y_pred):\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "evaluate_results(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
