{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_name = \".\"\n",
    "#project_name = \"jfreechart\"\n",
    "#project_name = \"argouml\"\n",
    "project_name = \"weka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      QualifiedName                     Name  Complexity   \n",
      "1                     <Package>weka                     weka           1  \\\n",
      "2        <Package>weka.associations        weka.associations           4   \n",
      "3  <Package>weka.attributeSelection  weka.attributeSelection           4   \n",
      "4         <Package>weka.classifiers         weka.classifiers           3   \n",
      "5   <Package>weka.classifiers.bayes   weka.classifiers.bayes           2   \n",
      "\n",
      "   Coupling  Size  Lack of Cohesion  #(C&I)    #C   #I     LOC     AC    EC   \n",
      "1         1     1                 1     2.0   2.0  0.0   204.0    1.0   1.0  \\\n",
      "2         3     3                 1    29.0  26.0  3.0  3917.0    8.0  19.0   \n",
      "3         4     4                 1    31.0  25.0  6.0  6166.0    8.0  21.0   \n",
      "4         4     3                 1    27.0  20.0  7.0  3186.0  114.0  24.0   \n",
      "5         2     3                 1     7.0   7.0  0.0  1640.0   37.0   6.0   \n",
      "\n",
      "     Abs    Ins     ND     WMC  \n",
      "1  0.000  0.500  0.500    74.0  \n",
      "2  0.276  0.704  0.020  1061.0  \n",
      "3  0.387  0.724  0.111  1681.0  \n",
      "4  0.704  0.174  0.122   877.0  \n",
      "5  0.000  0.140  0.860   448.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t')\n",
    "    return data\n",
    "\n",
    "def remove_empty_rows(data):\n",
    "    data = data.dropna(subset=['Name'])\n",
    "    return data\n",
    "\n",
    "def remove_unnamed_columns(data):\n",
    "    data = data.filter(regex='^(?!Unnamed.*)')\n",
    "    return data\n",
    "\n",
    "def remove_nan_columns(data):\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    return data\n",
    "\n",
    "def convert_ordinal_to_numerical(data):\n",
    "    ordinal_mapping = {\n",
    "        'low': 1,\n",
    "        'low-medium': 2,\n",
    "        'medium-high': 3,\n",
    "        'high': 4,\n",
    "        'very-high': 5\n",
    "    }\n",
    "    columns_to_convert = ['Complexity', 'Coupling', 'Size', 'Lack of Cohesion']\n",
    "    for column in columns_to_convert:\n",
    "        data[column] = data[column].replace(ordinal_mapping)\n",
    "    return data\n",
    "\n",
    "dataset_file = \"input/\" + project_name + \"/package-metrics-dataset.csv\"\n",
    "dataset = load_data(dataset_file)\n",
    "dataset = remove_empty_rows(dataset)\n",
    "dataset = remove_unnamed_columns(dataset)\n",
    "dataset = remove_nan_columns(dataset)\n",
    "dataset = convert_ordinal_to_numerical(dataset)\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the dataset: 89\n",
      "Number of Problematic Components in the dataset: 30\n"
     ]
    }
   ],
   "source": [
    "def label_problematic_component(row, problematic_class_threshold=3, highly_problematic_class_threshold=4):\n",
    "    if row['Complexity'] >= problematic_class_threshold and \\\n",
    "        row['Coupling'] >= problematic_class_threshold or \\\n",
    "        row['Lack of Cohesion'] >= problematic_class_threshold:\n",
    "        return 1\n",
    "    elif row['Complexity'] >= highly_problematic_class_threshold or \\\n",
    "        row['Coupling'] >= highly_problematic_class_threshold or \\\n",
    "        row['Lack of Cohesion'] >= highly_problematic_class_threshold or \\\n",
    "        row['Size'] >= highly_problematic_class_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Label the dataset\n",
    "dataset['Problematic'] = dataset.apply(label_problematic_component, axis=1)\n",
    "\n",
    "# Count the number of Problematic Classes\n",
    "num_dataset_rows = dataset.shape[0]\n",
    "num_problematic_classes = dataset[dataset['Problematic'] == 1].shape[0]\n",
    "print(f'Number of instances in the dataset: {num_dataset_rows}')\n",
    "print(f'Number of Problematic Components in the dataset: {num_problematic_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Name  #(C&I)    #C   #I     LOC     AC    EC    Abs   \n",
      "1                     weka     2.0   2.0  0.0   204.0    1.0   1.0  0.000  \\\n",
      "2        weka.associations    29.0  26.0  3.0  3917.0    8.0  19.0  0.276   \n",
      "3  weka.attributeSelection    31.0  25.0  6.0  6166.0    8.0  21.0  0.387   \n",
      "4         weka.classifiers    27.0  20.0  7.0  3186.0  114.0  24.0  0.704   \n",
      "5   weka.classifiers.bayes     7.0   7.0  0.0  1640.0   37.0   6.0  0.000   \n",
      "\n",
      "     Ins     ND     WMC  Problematic  \n",
      "1  0.500  0.500    74.0            0  \n",
      "2  0.704  0.020  1061.0            1  \n",
      "3  0.724  0.111  1681.0            1  \n",
      "4  0.174  0.122   877.0            1  \n",
      "5  0.140  0.860   448.0            0  \n"
     ]
    }
   ],
   "source": [
    "def remove_labeling_columns(data):\n",
    "    data = data.drop(['QualifiedName', 'Complexity', 'Coupling', 'Size', 'Lack of Cohesion'], axis=1)\n",
    "    return data\n",
    "\n",
    "dataset = remove_labeling_columns(dataset)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    X = data.drop(['Name', 'Problematic'], axis=1)\n",
    "    y = data['Problematic']\n",
    "    return X, y\n",
    "\n",
    "X, y = preprocess_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_decision_tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf, X_test, y_test\n",
    "\n",
    "clf, X_test, y_test = train_decision_tree(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(clf, X_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "y_pred = make_predictions(clf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[18  0]\n",
      " [ 3  6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92        18\n",
      "           1       1.00      0.67      0.80         9\n",
      "\n",
      "    accuracy                           0.89        27\n",
      "   macro avg       0.93      0.83      0.86        27\n",
      "weighted avg       0.90      0.89      0.88        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_results(y_test, y_pred):\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "evaluate_results(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
